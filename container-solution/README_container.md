# Container solution using SSH from container
Container for Falcon using SSH login inside container to submit sbatch script. This is an alternative way to the conda solution, if a container approach is favoured. First move the contents of this directory to the main one:

    >cd dunnart/
    >mv container-solution/* .

# Description
This workflow is for the reference genome assembly of 
the Fat-tailed Dunnart ([*Sminthopsis crassicaudata*](https://bie.ala.org.au/species/urn:lsid:biodiversity.org.au:afd.taxon:86ab9ebf-cbd1-49f8-9786-312407738477), as part of an [Australian Biocommons](https://www.biocommons.org.au/) collaboration. Raw sequences were generated by Stephen Frankenberg (University of Melbourne) as part of the [Oz Mammals Genomics](https://ozmammalsgenomics.com/) (OMG) Framework Initiative, and downloaded from the Bioplatforms Australia data portal.

The tool used in this workflow is:
    
[Pacific Biosciences assembly tool suite](https://github.com/PacificBiosciences/pb-assembly) (v0.0.8) - the pb-assembly tool suite covers the three main functions of Falcon; namely fc_run, fc_unzip and fc_phase.

|Tool | Documentation | Optimisation |
|-----------|--------------------------|------------------|
|pb-assembly (container-solution)  | README_container.md | As per paramater settings in each .cfg file |

## Releases
Pre-release 2nd July 2020

# Tutorials
## Run FALCON

Run FALCON with the sbatch script as follows:

    >sbatch --account=$PAWSEY_PROJECT sbatch_container.sh

Note: The sbatch_container.sh script contains the commands requried for setting up SSH keys from the compute node to then be able to submit the nextflow sbatch script.

## Check job progress

While FALCON is running, you can check the progress of your jobs.

#### fc_run

Jobs left:
    
    >ls 0-rawreads/daligner-chunks/ | wc -l

Jobs completed:

    >find 0-rawreads/daligner-runs/j_*/uow-00 -name "daligner.done" | wc -l

Stats for reads and pre-assembled reads:

    >DBstats raw_reads.db

    >DBstats 1-preads_ovl/build/preads.db 

Check pre-assembly performance:

    >cat 0-rawreads/report/pre_assembly_stats.json

Check assembly performance:

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 2-asm-falcon/p_ctg.fasta

#### fc_unzip 

Check haplotype resolution

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 3-unzip/all_p_ctg.fa 

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 3-unzip/all_h_ctg.fa

    >head 3-unzip/all_h_ctg.paf 

Check phase polishing

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 4-polish/cns-output/cns_p_ctg.fasta
   
    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 4-polish/cns-output/cns_h_ctg.fasta


#### fc_phase

See haplotig placement file
   
    >head 5-phase/placement-output/haplotig.placement

See final output stats 

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 5-phase/output/phased.0.fasta

    >singularity exec pbcore_1.7.1--py27_0.sif python pb-assembly/scripts/get_asm_stats.py 5-phase/output/phased.1.fasta

# Workflow infrastructure requirements

## Workflow manager
Nextflow (v19.10.0)

## Scheduler
SLURM

## Container engine
Singularity (v3.5.2)

## Environment
Miniconda3 - this environment will be activated from the workflow install. Independent install is not required.

## Hardware

# Workflow install
The following steps cover installation of the tools required for performing the de novo assembly of the fat-tailed Dunnart genome using Falcon on Pawsey's HPC system Zeus. The original Falcon documentation that this workflow is derived from can be found [here](https://github.com/PacificBiosciences/pb-assembly#tutorial).

### Start an interactive SLURM session
On Zeus, you will need to run your jobs on a work node; to do so, start an interactive session as follows:

    >salloc -n 1 -t 1:00:00

### Clone the Dunnart repository

The Dunnary repository contains the scripts and config files required for running the workflow. Clone it to your current working directory, e.g. /group/$PAWSEY_PROJECT/$USER

    >cd /group/$PAWSEY_PROJECT/$USER
    >git clone https://github.com/audreystott/dunnart.git
    >cd dunnart

### Download your raw sequencing data

You will need both the fasta and bam formats of the raw sequencing data. Download them to  your current working directory, which should be the `dunnart/` directory.

### Set the fasta and bam filenames in the run script

Ensure the filenames are set in the `falcon-container.sh` script. Do so by replacing the words `YOUR_FASTA_FILE_NAME` and `YOUR_BAM_FILE_NAME` with your raw filenames for the below commands. 

Note: Your fasta and bam filenames should have a suffix .subreads.fasta.gz and .subreads.bam, respectively.

    >sed -i "s|F1_bull_test.subreads.fasta.gz|YOUR_FASTA_FILE_NAME|g" falcon-container.sh
    >sed -i "s|F1_bull_test.subreads.bam|YOUR_BAM_FILE_NAME|g" falcon-container.sh 

### Set the HiC filename in the Nextflow script

Set your HiC filename in the Nextflow `main.nf` script. 

Note: The files should have the suffixes .HiC_R1.fastq.gz and .HiC_R2.fastq.gz.

You will replace the words `YOUR_HIC_FILE_NAME` with your HiC filename for the below. It should look similar to this - `sample1.HiC_R*.fastq.gz`.

    >sed -i "s|F1_bull_test.HiC_R*.fastq.gz|YOUR_HIC_FILE_NAME|g" main.nf

### Set up FALCON

Run the FALCON script falcon-container.sh.

    >bash falcon-container.sh    

### Exit the SLURM interactive session

Once the FALCON set up script has completed running, exit the session.

    >exit

## Workflow infrastructure requirements
- Pawsey [Zeus](https://www.pawsey.org.au/systems/zeus)
- More to add

# FAQ/Troubleshooting
## Note 1
We encountered a bug in the 2-asm_falcon ovlp_filtering stage, where preads.m4 had an erroneous '---' at the end of the file. We fixed this by following this github issue: https://github.com/PacificBiosciences/pbbioconda/issues/294 

## Note 2
It seems the fofn files need the absoluate path of the bam/fasta files so they can be found from anywhere.

# License(s)

# Acknowledgements/citations/credits
We would like to acknowledge the contribution of the Oz Mammals Genomics Initiative [consortium](https://ozmammalsgenomics.com/consortium/) in the generation of data used for the Dunnart reference genome assembly. The Initiative is supported by funding from Bioplatforms Australia through the Australian Government National Collaborative Research Infrastructure Strategy (NCRIS). 

The first release is credited to Pawsey Supercomputing Centre in collaboration with the Australian Biocommons.